#coding=utf-8

import numpy as np
from sklearn.datasets import make_classification
########生成样本数据
x,y=make_classification(10000,n_features=30,n_informative=2,n_redundant=2,n_classes=2,random_state=0)
#生成10000个样本，每个样本有30个特征,其中Y 为01的集合
#y[:,None] 转置Y ，如果Y 是矩阵可以直接使用y.T

########使用DF生成图表
from pandas import DataFrame
df=DataFrame(np.hstack((x,y[:,None])),columns=range(30)+['class'])
print df[:3] #很显然是有属性和类别的

#######绘制相关系图
import matplotlib.pyplot as plt
import seaborn as sns
_=sns.pairplot(df[:50],vars=[2,8,12,18,21,29],hue='class',size=1.5)
'''
样本数据，属性（列），分类依据是根据：class，图片大小
'''
plt.show()

plt.figure(figsize=(12,10))
_=sns.corrplot(df,annot=False)
plt.show()
#如果是绘制属性之间的关系图可以使用pandas来计算

########使用线性内核的支持向量机
from sklearn.svm import LinearSVC
from sklearn.learning_curve import learning_curve

#learning_curve(LinearSVC(C=10.0),x,y,train_sizes=np.linspace(0.1,1.0,5),cv=3)

def plot_learning_curve(estimator,title,x,y,ylim=None,cv=None,train_sizes=np.linspace(0.1,1.0,5)):
    '''
    画出学习曲线
    :param estimator: 使用的机器学习的算法，包括其中的参数
    :param title: 标题
    :param x: 输入的样本属性
    :param y: 输入样本的类别
    :param ylim: 纵坐标的最大值和最小值
    :param cv: 样本分成的份数，其中1分交叉训练集
    :param train_sizes:
    :return: 返回学习曲线
    '''
    plt.figure()
    train_sizes,train_scores,test_scores=learning_curve(estimator,x,y,train_sizes=train_sizes,cv=3)
    train_scores_mean=np.mean(train_scores,axis=1)
    train_scores_std=np.std(train_scores,axis=1)
    test_scores_mean=np.mean(test_scores,axis=1)
    test_scores_std=np.std(test_scores,axis=1)

    plt.fill_between(train_sizes,train_scores_mean+train_scores_std,
                     train_scores_mean-train_scores_std,alpha=0.1,color='g')
    #绘制学习曲线的训练数据的得分的上下限为平均值+/-方差，绘制条状线
    plt.fill_between(train_sizes,test_scores_mean+test_scores_std,
                     test_scores_mean-test_scores_std,alpha=0.1,color='r')
    plt.plot(train_sizes,train_scores_mean,'o-',color='g',label='training score')
    plt.plot(train_sizes,test_scores_mean,'o-',color='r',label='cross validation score')

    plt.xlabel('training examples')
    plt.ylabel('score')
    plt.legend(loc='best')
    plt.grid('on')
    #plt.ylim(ylim)
    plt.title(title)
    plt.show()

plot_learning_curve(LinearSVC(C=3),'LinearSVC(C=3)',x,y,train_sizes=np.linspace(0.1,1,10))
#其中更改trainsize就是更改训练样本数目

plot_learning_curve(LinearSVC(C=10),'LinearSVC(C=3)',x[:,[18,21,29]],y,train_sizes=np.linspace(0.1,1,10))
#选取重要的特征点

from sklearn.grid_search import GridSearchCV
#修改参数值
estm=GridSearchCV(LinearSVC(),param_grid={'C':[0.1,0.5,1,3,5,10]})
estm.fit(x[:500],y[:500])
print estm.fit(x[:500],y[:500]).best_params_
print estm.fit(x[:500],y[:500]).best_score_
#评估属性的权重
estm2=LinearSVC(C=10,penalty='l1',dual=False)
estm2.fit(x[:500],y[:500])
print estm2.coef_

#使用非线性内核
from sklearn.svm import SVC
plot_learning_curve(SVC(C=2.5,kernel='rbf',gamma=1.0),'LinearSVC(C=3)',x,y,train_sizes=np.linspace(0.1,1,10))
